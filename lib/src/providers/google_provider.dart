import 'dart:async';
import 'dart:convert';
import 'dart:io';

import '../core/provider_registry.dart';
import '../models/additional_params.dart';
import '../models/provider_response.dart';
import '../models/ai_provider_metadata.dart';
import '../models/ai_system_prompt.dart';
import '../models/ai_audio_params.dart';

import '../utils/json_utils.dart' as json_utils;
import 'package:http/http.dart' as http;
import '../utils/logger.dart';
import '../models/ai_capability.dart';
import '../models/audio_models.dart';
import '../core/base_provider.dart';

/// Google Gemini provider - Complete Gemini API integration with native TTS/STT
class GoogleProvider extends BaseProvider {
  GoogleProvider(super.config);

  static void register() {
    ProviderRegistry.instance.registerConstructor(
        'google', (final config) => GoogleProvider(config));
  }

  @override
  String get providerId => 'google';

  @override
  AIProviderMetadata createMetadata() {
    return AIProviderMetadata(
      providerId: providerId,
      providerName: config.displayName,
      company: 'Google',
      version: '1.0.0',
      description: config.description,
      supportedCapabilities: config.capabilities,
      defaultModels: config.defaults,
      availableModels: config.models,
      rateLimits: {
        'requests_per_minute': config.rateLimits.requestsPerMinute,
        'tokens_per_minute': config.rateLimits.tokensPerMinute,
      },
      requiresAuthentication: true,
      requiredConfigKeys: config.apiSettings.requiredEnvKeys,
      maxContextTokens: config.configuration.maxContextTokens,
      maxOutputTokens: config.configuration.maxOutputTokens,
      supportsStreaming: config.configuration.supportsStreaming,
      supportsFunctionCalling: config.configuration.supportsFunctionCalling,
    );
  }

  @override
  Map<String, String> buildAuthHeaders() {
    return {'Content-Type': 'application/json', 'x-goog-api-key': apiKey};
  }

  @override
  Future<List<String>?> fetchModelsFromAPI() async {
    try {
      final url = Uri.parse(getEndpointUrl('models'));
      final response =
          await http.Client().get(url, headers: buildAuthHeaders());

      if (isSuccessfulResponse(response.statusCode)) {
        final data = jsonDecode(response.body);
        final models = (data['models'] as List<dynamic>)
            .map((final model) =>
                (model['name'] as String).replaceFirst('models/', ''))
            .where((final model) => isValidModelForProvider(model))
            .toList();

        // Apply priority-based sorting
        models.sort(compareModels);
        return models;
      }
      return null;
    } on Exception catch (_) {
      return null;
    }
  }

  @override
  int compareModels(final String a, final String b) {
    final priorityA = _getModelPriority(a);
    final priorityB = _getModelPriority(b);
    return priorityA != priorityB
        ? priorityA.compareTo(priorityB)
        : b.compareTo(a);
  }

  int _getModelPriority(final String model) {
    final m = model.toLowerCase();
    if (m.contains('experimental')) return 1;
    if (m.contains('flash')) return 2;
    if (m.contains('pro')) return 3;
    if (m.contains('nano')) return 4;
    return 99;
  }

  @override
  Future<ProviderResponse> sendMessage({
    required final AISystemPrompt systemPrompt,
    required final AICapability capability,
    final String? model,
    final String? imageBase64,
    final String? imageMimeType,
    final AdditionalParams? additionalParams,
    final String? voice,
  }) async {
    switch (capability) {
      case AICapability.textGeneration:
      case AICapability.imageAnalysis:
        return _sendTextRequest(
            systemPrompt, model, imageBase64, imageMimeType, additionalParams);
      case AICapability.imageGeneration:
        return _sendImageGenerationRequest(
            systemPrompt, model, additionalParams);
      case AICapability.audioGeneration:
        return _sendTTSRequest(systemPrompt, model, additionalParams, voice);
      case AICapability.audioTranscription:
        return _sendTranscriptionRequest(
          imageBase64 ?? '',
          systemPrompt,
          model,
        );
      case AICapability.realtimeConversation:
        return _handleRealtimeRequest(systemPrompt, model, additionalParams);
    }
  }

  Future<ProviderResponse> _sendTextRequest(
    final AISystemPrompt systemPrompt,
    final String? model,
    final String? imageBase64,
    final String? imageMimeType,
    final AdditionalParams? additionalParams,
  ) async {
    try {
      final selectedModel =
          model ?? getDefaultModel(AICapability.textGeneration);
      if (selectedModel == null || !isValidModelForProvider(selectedModel)) {
        return ProviderResponse(
            text: 'Error: Invalid or missing model for Google provider');
      }

      final history = systemPrompt.history ?? [];
      final contents = <Map<String, dynamic>>[];

      // Add conversation history
      for (int i = 0; i < history.length; i++) {
        final role = history[i]['role'] == 'assistant' ? 'model' : 'user';
        final parts = <Map<String, dynamic>>[];

        parts.add({'text': history[i]['content'] ?? ''});

        // Add image if present and it's the last message
        if (imageBase64 != null &&
            imageBase64.isNotEmpty &&
            i == history.length - 1) {
          final mimeType = imageMimeType ?? defaults['image_mime_type'];
          parts.add({
            'inline_data': {'mime_type': mimeType, 'data': imageBase64},
          });
        }

        contents.add({'role': role, 'parts': parts});
      }

      // ‚ú® Build system instruction from AISystemPrompt
      final systemParts = <String>[];

      // Include context
      final contextJson = systemPrompt.toJson();
      final contextData = contextJson['context'];
      if (contextData != null) {
        final contextStr =
            contextData is String ? contextData : jsonEncode(contextData);
        if (contextStr.isNotEmpty &&
            contextStr != '{}' &&
            contextStr != 'null') {
          systemParts.add('Context:\n$contextStr');
        }
      }

      // Include instructions from systemPrompt.instructions
      if (systemPrompt.instructions.isNotEmpty) {
        final instructionsStr = jsonEncode(systemPrompt.instructions);
        if (instructionsStr.isNotEmpty &&
            instructionsStr != '{}' &&
            instructionsStr != 'null') {
          // If instructions contain a 'raw' key, use that directly
          if (systemPrompt.instructions.containsKey('raw')) {
            systemParts
                .add('Instructions:\n${systemPrompt.instructions['raw']}');
          } else {
            systemParts.add('Instructions:\n$instructionsStr');
          }
        }
      }

      final bodyMap = <String, dynamic>{
        'contents': contents,
        'generationConfig': {
          'maxOutputTokens': config.configuration.maxOutputTokens,
          'temperature': 0.7,
        },
      };

      // ‚ú® Add systemInstruction at top level if we have system content
      if (systemParts.isNotEmpty) {
        bodyMap['systemInstruction'] = {
          'parts': [
            {'text': systemParts.join('\n\n')}
          ]
        };
        AILogger.d(
            '[GoogleProvider] üéØ Using native systemInstruction with ${systemParts.length} parts');
      }

      final url = Uri.parse(
          getEndpointUrl('chat').replaceAll('{model}', selectedModel));
      final response = await http.Client()
          .post(url, headers: buildAuthHeaders(), body: jsonEncode(bodyMap));

      if (isSuccessfulResponse(response.statusCode)) {
        return _processGeminiResponse(jsonDecode(response.body));
      } else {
        final hasMoreKeys = handleApiError(
            response.statusCode, response.body, 'text_generation');
        if (!hasMoreKeys) {
          throw StateError(
              'All Google AI API keys have been exhausted - no retry needed');
        }
        throw Exception(
            '${response.statusCode} ${response.reasonPhrase ?? ''}: ${response.body}');
      }
    } on Exception catch (e) {
      AILogger.e('[GoogleProvider] Text request failed: $e');
      rethrow;
    }
  }

  ProviderResponse _processGeminiResponse(final Map<String, dynamic> data) {
    try {
      final candidates = data['candidates'] as List<dynamic>?;
      if (candidates != null && candidates.isNotEmpty) {
        final candidate = candidates.first as Map<String, dynamic>;

        // üéØ NUEVO: Verificar finishReason para detectar rechazos de pol√≠ticas
        final finishReason = candidate['finishReason'] as String?;
        if (finishReason != null) {
          switch (finishReason) {
            case 'IMAGE_OTHER':
              AILogger.w(
                  '[GoogleProvider] ‚ö†Ô∏è Google rechaz√≥ la imagen por pol√≠ticas de contenido (IMAGE_OTHER)');
              return ProviderResponse(
                text:
                    'Error: Google rechaz√≥ generar la imagen por pol√≠ticas de contenido. Intenta con una descripci√≥n menos espec√≠fica.',
              );
            case 'SAFETY':
              AILogger.w(
                  '[GoogleProvider] ‚ö†Ô∏è Google rechaz√≥ la imagen por seguridad (SAFETY)');
              return ProviderResponse(
                text:
                    'Error: Google rechaz√≥ generar la imagen por motivos de seguridad.',
              );
            case 'RECITATION':
              AILogger.w(
                  '[GoogleProvider] ‚ö†Ô∏è Google rechaz√≥ la imagen por recitaci√≥n (RECITATION)');
              return ProviderResponse(
                text:
                    'Error: Google rechaz√≥ generar la imagen por pol√≠ticas de recitaci√≥n.',
              );
            case 'STOP':
              // STOP es normal, continuar procesando
              break;
            default:
              AILogger.w(
                  '[GoogleProvider] ‚ö†Ô∏è Google finaliz√≥ con raz√≥n desconocida: $finishReason');
              break;
          }
        }

        final content = candidate['content'] as Map<String, dynamic>?;

        if (content != null) {
          final parts = content['parts'] as List<dynamic>?;
          if (parts != null && parts.isNotEmpty) {
            // Check for image data first (Google uses camelCase 'inlineData')
            final imagePart = parts.firstWhere(
              (final part) => part is Map && part.containsKey('inlineData'),
              orElse: () => null,
            );

            if (imagePart != null) {
              final inlineData =
                  imagePart['inlineData'] as Map<String, dynamic>;
              final imageBase64 = inlineData['data'] as String?;

              if (imageBase64 != null && imageBase64.isNotEmpty) {
                // üî• CAPTURAR EL TEXTO REAL DE GEMINI como "revised prompt"
                final textPart = parts.firstWhere(
                  (final part) => part is Map && part.containsKey('text'),
                  orElse: () => null,
                );

                String geminiText = '';
                String revisedPrompt = '';

                if (textPart != null) {
                  final fullText = textPart['text'] as String? ?? '';

                  AILogger.d(
                      '[GoogleProvider] üñºÔ∏è Processing image response text (${fullText.length} chars)');

                  // Extraer JSON estructurado de la respuesta
                  final jsonExtracted = json_utils.extractJsonBlock(fullText);

                  if (!jsonExtracted.containsKey('raw')) {
                    // JSON v√°lido extra√≠do - usar solo el campo description
                    final description = jsonExtracted['description'];
                    final response = jsonExtracted['response'];

                    // Asegurarse de que description no contenga delimitadores markdown
                    if (description != null) {
                      revisedPrompt = description.toString().trim();
                      AILogger.d(
                          '[GoogleProvider] ‚úÖ Extracted description (${revisedPrompt.length} chars)');
                    } else {
                      AILogger.w(
                          '[GoogleProvider] ‚ö†Ô∏è No description field in extracted JSON');
                      revisedPrompt = '';
                    }

                    if (response != null) {
                      geminiText = response.toString().trim();
                    } else {
                      geminiText = 'Image generated successfully';
                    }
                  } else {
                    // Fallback: si no hay JSON v√°lido, usar texto completo como descripci√≥n
                    final rawText =
                        jsonExtracted['raw']?.toString() ?? fullText;
                    revisedPrompt = rawText;
                    geminiText = 'Image generated successfully';
                    AILogger.w(
                        '[GoogleProvider] ‚ö†Ô∏è Could not extract JSON, using raw text as prompt');
                  }
                } else {
                  AILogger.w(
                      '[GoogleProvider] ‚ö†Ô∏è No text part found in image response');
                }

                return ProviderResponse(
                  text: geminiText.isNotEmpty
                      ? geminiText
                      : 'Image generated successfully',
                  prompt: revisedPrompt, // ‚úÖ Solo la descripci√≥n limpia
                  imageBase64: imageBase64,
                );
              }
            }

            // Fallback to text content
            final textPart = parts.firstWhere(
              (final part) => part is Map && part.containsKey('text'),
              orElse: () => null,
            );

            if (textPart != null) {
              return ProviderResponse(text: textPart['text'] as String);
            }
          }
        }

        // üéØ NUEVO: Si tenemos finishReason problem√°tico y sin contenido, reportar espec√≠ficamente
        if (finishReason == 'IMAGE_OTHER' ||
            finishReason == 'SAFETY' ||
            finishReason == 'RECITATION') {
          return ProviderResponse(
            text:
                'Error: Google no pudo generar la imagen (finishReason: $finishReason)',
          );
        }
      }

      return ProviderResponse(text: 'No response content found');
    } on Exception catch (e) {
      AILogger.e('[GoogleProvider] Error processing response: $e');
      return ProviderResponse(text: 'Error processing response: $e');
    }
  }

  Future<ProviderResponse> _sendImageGenerationRequest(
    final AISystemPrompt systemPrompt,
    final String? model,
    final AdditionalParams? additionalParams,
  ) async {
    final history = systemPrompt.history ?? [];
    final prompt = history.isNotEmpty ? history.last['content'] ?? '' : '';
    if (prompt.isEmpty) {
      return ProviderResponse(
          text: 'Error: No prompt provided for image generation.');
    }

    try {
      final selectedModel =
          model ?? getDefaultModel(AICapability.imageGeneration);
      if (selectedModel == null) {
        return ProviderResponse(
            text: 'Error: No model configured for image generation');
      }

      final contents = <Map<String, dynamic>>[];

      // ‚ú® 1. Preparar el prompt del usuario (con imagen base si est√° disponible)
      // ‚úÖ Obtener imagen base para edici√≥n si est√° en additionalParams
      final sourceImageBase64 =
          additionalParams?.imageParams?.sourceImageBase64;

      // Modificar el prompt para que Gemini tambi√©n genere una descripci√≥n detallada
      final isEditing =
          sourceImageBase64 != null && sourceImageBase64.isNotEmpty;

      final enhancedPrompt = isEditing
          ? '''$prompt

After editing the image, provide your response as a JSON object with the following structure:
{
  "description": "Detailed description of the changes made and final result including what was modified, visual elements, composition, colors, lighting, style and key features",
  "response": "Brief confirmation message about the edit"
}'''
          : '''$prompt

After generating the image, provide your response as a JSON object with the following structure:
{
  "description": "Detailed description of what you created including visual elements, composition, colors, lighting, style, key features and artistic choices",
  "response": "Brief confirmation message about the generation"
}''';

      final userParts = <Map<String, dynamic>>[
        {'text': enhancedPrompt}
      ];
      if (sourceImageBase64 != null && sourceImageBase64.isNotEmpty) {
        final imageData = sourceImageBase64.startsWith('data:')
            ? sourceImageBase64.split(',').last
            : sourceImageBase64;

        userParts.add({
          'inline_data': {
            'mime_type': 'image/jpeg',
            'data': imageData,
          },
        });

        AILogger.d(
            '[GoogleProvider] üñºÔ∏è Image editing mode: usando imagen base para edici√≥n');
      }

      contents.add({
        'role': 'user',
        'parts': userParts,
      });

      // ‚ú® 2. Build systemInstruction from AISystemPrompt (mismo enfoque que _sendTextRequest)
      final contextJson = systemPrompt.toJson();
      final systemParts = <String>[];

      // Include context
      final contextData = contextJson['context'];
      if (contextData != null) {
        final contextStr =
            contextData is String ? contextData : jsonEncode(contextData);
        if (contextStr.isNotEmpty &&
            contextStr != '{}' &&
            contextStr != 'null') {
          systemParts.add('Context:\n$contextStr');
        }
      }

      // Include instructions from systemPrompt.instructions
      if (systemPrompt.instructions.isNotEmpty) {
        final instructionsStr = jsonEncode(systemPrompt.instructions);
        if (instructionsStr.isNotEmpty &&
            instructionsStr != '{}' &&
            instructionsStr != 'null') {
          // If instructions contain a 'raw' key, use that directly
          if (systemPrompt.instructions.containsKey('raw')) {
            systemParts
                .add('Instructions:\n${systemPrompt.instructions['raw']}');
          } else {
            systemParts.add('Instructions:\n$instructionsStr');
          }
        }
      }

      final bodyMap = <String, dynamic>{
        'contents': contents,
        'generationConfig': {
          'maxOutputTokens': config.configuration.maxOutputTokens,
          'temperature': 0.7,
        },
      };

      // ‚ú® Add systemInstruction at top level if we have system content
      if (systemParts.isNotEmpty) {
        bodyMap['systemInstruction'] = {
          'parts': [
            {'text': systemParts.join('\n\n')}
          ]
        };
        AILogger.d(
            '[GoogleProvider] üéØ Image generation using native systemInstruction with ${systemParts.length} parts');
      }

      final url = Uri.parse(
          getEndpointUrl('chat').replaceAll('{model}', selectedModel));
      final response = await http.Client()
          .post(url, headers: buildAuthHeaders(), body: jsonEncode(bodyMap));

      if (isSuccessfulResponse(response.statusCode)) {
        final responseBody = jsonDecode(response.body);
        return _processGeminiResponse(responseBody);
      } else {
        return ProviderResponse(
            text: 'Error generating image: ${response.statusCode}');
      }
    } on Exception catch (e) {
      AILogger.e('[GoogleProvider] Image generation failed: $e');
      return ProviderResponse(text: 'Error generating image: $e');
    }
  }

  /// Native Gemini TTS using generateContent
  Future<ProviderResponse> _sendTTSRequest(
    final AISystemPrompt systemPrompt,
    final String? model,
    final AdditionalParams? additionalParams,
    final String? voice,
  ) async {
    final history = systemPrompt.history ?? [];
    final text = history.isNotEmpty ? history.last['content'] ?? '' : '';
    if (text.isEmpty) {
      return ProviderResponse(text: 'Error: No text provided for TTS.');
    }

    try {
      final selectedModel =
          model ?? getDefaultModel(AICapability.audioGeneration);
      // Use voice parameter directly with fallback to getDefaultVoice()
      final selectedVoice = voice ??
          getDefaultVoice(); // Crear AiAudioParams desde additionalParams para usar los nuevos campos
      final audioParams =
          additionalParams?.audioParams ?? const AiAudioParams();

      // Construir prompt nativo de TTS con par√°metros avanzados
      final ttsPrompt =
          _buildAdvancedTtsPrompt(text, selectedVoice, audioParams);

      final contents = [
        {
          'role': 'user',
          'parts': [
            {'text': ttsPrompt}
          ],
        }
      ];

      final generationConfig = <String, dynamic>{
        'response_modalities': ['AUDIO'], // Configurar para respuesta de audio
        'speech_config': {
          'voice_config': {
            'prebuilt_voice_config': {
              'voice_name': selectedVoice,
            }
          }
        }
      };

      // A√±adir temperature si est√° especificada
      if (audioParams.temperature != null) {
        generationConfig['temperature'] = audioParams.temperature;
      }

      final bodyMap = {
        'contents': contents,
        'generationConfig': generationConfig,
      };

      // Validation: ensure we have a model
      if (selectedModel == null || selectedModel.isEmpty) {
        throw StateError(
            'No TTS model available in Google provider configuration');
      }

      final url = Uri.parse(
          getEndpointUrl('tts_generate').replaceAll('{model}', selectedModel));
      final response = await http.Client()
          .post(url, headers: buildAuthHeaders(), body: jsonEncode(bodyMap));

      if (isSuccessfulResponse(response.statusCode)) {
        final responseBody = jsonDecode(response.body);

        // Extraer audio de la respuesta de Gemini
        String? audioBase64;
        if (responseBody['candidates'] != null &&
            responseBody['candidates'].isNotEmpty) {
          final candidate = responseBody['candidates'][0];
          if (candidate['content'] != null &&
              candidate['content']['parts'] != null &&
              candidate['content']['parts'].isNotEmpty) {
            final part = candidate['content']['parts'][0];
            if (part['inlineData'] != null &&
                part['inlineData']['data'] != null) {
              audioBase64 = part['inlineData']['data'];
              AILogger.d(
                  '[GoogleProvider] Extracted audio base64 data (${audioBase64?.length ?? 0} chars)');
              // Debug: verify base64 starts correctly
              if (audioBase64 != null && audioBase64.isNotEmpty) {
                final first50 = audioBase64.length > 50
                    ? audioBase64.substring(0, 50)
                    : audioBase64;
                AILogger.d('[GoogleProvider] Base64 preview: $first50...');
              }
            }
          }
        }

        return ProviderResponse(
          text: 'Audio generated successfully with Gemini TTS',
          audioBase64: audioBase64 ?? '',
        );
      } else {
        AILogger.e(
            '[GoogleProvider] Audio generation failed with HTTP ${response.statusCode}: ${response.body}');
        throw HttpException(
            'HTTP ${response.statusCode}: ${response.reasonPhrase ?? 'Audio generation failed'}');
      }
    } on Exception catch (e) {
      AILogger.e('[GoogleProvider] TTS request failed: $e');
      return ProviderResponse(text: 'Error connecting to Gemini TTS: $e');
    }
  }

  /// Construye un prompt TTS avanzado usando los nuevos par√°metros de audio
  String _buildAdvancedTtsPrompt(
    final String text,
    final String voice,
    final AiAudioParams audioParams,
  ) {
    final promptBuilder = StringBuffer();

    promptBuilder.write('''
Please generate speech audio for the following text using voice "$voice":
"$text"

Requirements:
- Use natural intonation and pacing
- Clear pronunciation''');

    // A√±adir idioma si est√° especificado
    if (audioParams.language != null) {
      promptBuilder.write('\n- Speak in ${audioParams.language} language');
    }

    // A√±adir acento personalizado si est√° especificado
    if (audioParams.accent != null) {
      promptBuilder
          .write('\n- Use accent/pronunciation style: ${audioParams.accent}');
    }

    // A√±adir emoci√≥n personalizada si est√° especificada
    if (audioParams.emotion != null) {
      promptBuilder.write('\n- Emotional expression: ${audioParams.emotion}');
    }

    // A√±adir velocidad si no es la por defecto
    if (audioParams.speed != 1.0) {
      final speedDescription = audioParams.speed > 1.0
          ? 'faster than normal (${audioParams.speed}x speed)'
          : 'slower than normal (${audioParams.speed}x speed)';
      promptBuilder.write('\n- Speaking pace: $speedDescription');
    }

    // A√±adir formato de audio (Google siempre genera PCM internamente)
    promptBuilder
        .write('\n- Audio format preference: ${audioParams.audioFormat}');

    return promptBuilder.toString();
  }

  /// Native Gemini STT using generateContent
  Future<ProviderResponse> _sendTranscriptionRequest(
    final String audioBase64,
    final AISystemPrompt systemPrompt,
    final String? model,
  ) async {
    if (audioBase64.isEmpty) {
      return ProviderResponse(
          text: 'Error: No audio data provided for transcription.');
    }

    try {
      final selectedModel =
          model ?? getDefaultModel(AICapability.audioTranscription);

      // Construir prompt desde Context
      final promptText = _buildPromptFromContext(systemPrompt);
      final transcriptionPrompt = promptText.isNotEmpty
          ? promptText
          : 'Please transcribe this audio file to text. Provide only the transcribed text without additional comments.';

      // Native Gemini STT with multimodal input
      final contents = [
        {
          'role': 'user',
          'parts': [
            {'text': transcriptionPrompt},
            {
              'inline_data': {
                'mime_type': 'audio/wav',
                'data': audioBase64,
              }
            }
          ],
        }
      ];

      final bodyMap = {
        'contents': contents,
        'generationConfig': {
          'maxOutputTokens': 8192,
          'temperature': 0.1, // Very low temperature for accurate transcription
        },
      };

      // Validation: ensure we have a model
      if (selectedModel == null || selectedModel.isEmpty) {
        throw StateError(
            'No STT model available in Google provider configuration');
      }

      final url = Uri.parse(getEndpointUrl('stt_transcribe')
          .replaceAll('{model}', selectedModel));
      final response = await http.Client()
          .post(url, headers: buildAuthHeaders(), body: jsonEncode(bodyMap));

      if (isSuccessfulResponse(response.statusCode)) {
        return _processGeminiResponse(jsonDecode(response.body));
      } else {
        return ProviderResponse(
            text: 'Error transcribing audio: ${response.statusCode}');
      }
    } on Exception catch (e) {
      AILogger.e('[GoogleProvider] STT request failed: $e');
      return ProviderResponse(text: 'Error connecting to Gemini STT: $e');
    }
  }

  /// Handle realtime conversation setup
  Future<ProviderResponse> _handleRealtimeRequest(
    final AISystemPrompt systemPrompt,
    final String? model,
    final AdditionalParams? additionalParams,
  ) async {
    if (!supportsCapability(AICapability.realtimeConversation)) {
      return ProviderResponse(
          text:
              'Realtime conversation not supported by this Google provider configuration');
    }

    final realtimeModel =
        model ?? getDefaultModel(AICapability.realtimeConversation);
    return ProviderResponse(
      text:
          'Realtime conversation session configured successfully. Provider: google, Model: $realtimeModel, History: ${systemPrompt.history?.length ?? 0} messages',
    );
  }

  /// DEPRECATED: Legacy method - use AI.speak() instead
  /// Voice is now managed through AI.setSelectedVoiceForProvider()
  @Deprecated(
      'Use AI.speak() with voice management via AI.setSelectedVoiceForProvider()')
  Future<ProviderResponse> generateAudio({
    required final String text,
    final String? voice,
    final String? model,
    final AdditionalParams? additionalParams,
  }) async {
    // Create temporary AISystemPrompt for TTS
    final systemPrompt = AISystemPrompt(
      context: '',
      dateTime: DateTime.now(),
      instructions: {},
      history: [
        {'role': 'user', 'content': text}
      ],
    );

    return _sendTTSRequest(
      systemPrompt,
      model,
      additionalParams,
      voice,
    );
  }

  Future<ProviderResponse> transcribeAudio({
    required final String audioBase64,
    final String? model,
    final AISystemPrompt? systemPrompt,
  }) async {
    final effectiveContext = systemPrompt ??
        AISystemPrompt(
          context: {'task': 'audio_transcription'},
          dateTime: DateTime.now(),
          instructions: {},
        );
    return _sendTranscriptionRequest(audioBase64, effectiveContext, model);
  }

  /// [REMOVED] createRealtimeClient - Replaced by HybridConversationService
  /// Use HybridConversationService for real-time conversation features

  bool supportsRealtimeForModel(final String? model) {
    if (model == null) {
      return supportsCapability(AICapability.realtimeConversation);
    }
    final realtimeModels =
        availableModels[AICapability.realtimeConversation] ?? [];
    return realtimeModels.contains(model) || model.contains('gemini');
  }

  List<String> getAvailableRealtimeModels() {
    return supportsCapability(AICapability.realtimeConversation)
        ? availableModels[AICapability.realtimeConversation] ?? []
        : [];
  }

  bool get supportsRealtime =>
      supportsCapability(AICapability.realtimeConversation);

  String? get defaultRealtimeModel => supportsRealtime
      ? getDefaultModel(AICapability.realtimeConversation)
      : null;

  // Native Gemini voices - All 30 official voices from Gemini API documentation
  static final List<VoiceInfo> _availableVoices = [
    // Row 1: Brilliant, Optimistic, Informative
    const VoiceInfo(
        id: 'Zephyr',
        name: 'Zephyr',
        language: 'multi',
        gender: VoiceGender.neutral,
        description: 'Brilliant voice'),
    const VoiceInfo(
        id: 'Puck',
        name: 'Puck',
        language: 'multi',
        gender: VoiceGender.neutral,
        description: 'Optimistic voice'),
    const VoiceInfo(
        id: 'Charon',
        name: 'Charon',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Informative voice'),

    // Row 2: Firm, Exciting, Youthful
    const VoiceInfo(
        id: 'Kore',
        name: 'Kore',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Firm voice'),
    const VoiceInfo(
        id: 'Fenrir',
        name: 'Fenrir',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Exciting voice'),
    const VoiceInfo(
        id: 'Leda',
        name: 'Leda',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Youthful voice'),

    // Row 3: Firm, Breezy, Quiet
    const VoiceInfo(
        id: 'Orus',
        name: 'Orus',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Firm voice'),
    const VoiceInfo(
        id: 'Aoede',
        name: 'Aoede',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Breezy voice'),
    const VoiceInfo(
        id: 'Callirrhoe',
        name: 'Callirrhoe',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Quiet voice'),

    // Row 4: Bright, Breathy, Clear
    const VoiceInfo(
        id: 'Autonoe',
        name: 'Autonoe',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Bright voice'),
    const VoiceInfo(
        id: 'Enceladus',
        name: 'Enceladus',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Breathy voice'),
    const VoiceInfo(
        id: 'Iapetus',
        name: 'Iapetus',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Clear voice'),

    // Row 5: Relaxed, Soft, Soft
    const VoiceInfo(
        id: 'Umbriel',
        name: 'Umbriel',
        language: 'multi',
        gender: VoiceGender.neutral,
        description: 'Relaxed voice'),
    const VoiceInfo(
        id: 'Algieba',
        name: 'Algieba',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Soft voice'),
    const VoiceInfo(
        id: 'Despina',
        name: 'Despina',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Soft voice'),

    // Row 6: Clear, Sandy, Informative
    const VoiceInfo(
        id: 'Erinome',
        name: 'Erinome',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Clear voice'),
    const VoiceInfo(
        id: 'Algenib',
        name: 'Algenib',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Sandy voice'),
    const VoiceInfo(
        id: 'Rasalgethi',
        name: 'Rasalgethi',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Informative voice'),

    // Row 7: Optimistic, Soft, Firm
    const VoiceInfo(
        id: 'Laomedeia',
        name: 'Laomedeia',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Optimistic voice'),
    const VoiceInfo(
        id: 'Achernar',
        name: 'Achernar',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Soft voice'),
    const VoiceInfo(
        id: 'Alnilam',
        name: 'Alnilam',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Firm voice'),

    // Row 8: Even, Mature, Forward
    const VoiceInfo(
        id: 'Schedar',
        name: 'Schedar',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Even voice'),
    const VoiceInfo(
        id: 'Gacrux',
        name: 'Gacrux',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Mature voice'),
    const VoiceInfo(
        id: 'Pulcherrima',
        name: 'Pulcherrima',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Forward voice'),

    // Row 9: Friendly, Casual, Soft
    const VoiceInfo(
        id: 'Achird',
        name: 'Achird',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Friendly voice'),
    const VoiceInfo(
        id: 'Zubenelgenubi',
        name: 'Zubenelgenubi',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Casual voice'),
    const VoiceInfo(
        id: 'Vindemiatrix',
        name: 'Vindemiatrix',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Soft voice'),

    // Row 10: Animated, Knowledgeable, Warm
    const VoiceInfo(
        id: 'Sadachbia',
        name: 'Sadachbia',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Animated voice'),
    const VoiceInfo(
        id: 'Sadaltager',
        name: 'Sadaltager',
        language: 'multi',
        gender: VoiceGender.male,
        description: 'Knowledgeable voice'),
    const VoiceInfo(
        id: 'Sulafat',
        name: 'Sulafat',
        language: 'multi',
        gender: VoiceGender.female,
        description: 'Warm voice'),
  ];

  Future<List<VoiceInfo>> getAvailableVoices() async => _availableVoices;

  VoiceGender getVoiceGender(final String voiceName) {
    return _availableVoices
        .firstWhere(
          (final v) => v.name.toLowerCase() == voiceName.toLowerCase(),
          orElse: () => const VoiceInfo(
              id: 'default',
              name: 'default',
              language: 'multi',
              gender: VoiceGender.neutral),
        )
        .gender;
  }

  List<String> getVoiceNames() =>
      _availableVoices.map((final v) => v.name).toList();

  bool isValidVoice(final String voiceName) => _availableVoices
      .any((final v) => v.name.toLowerCase() == voiceName.toLowerCase());

  /// Construye prompt para transcripci√≥n desde Context
  String _buildPromptFromContext(final AISystemPrompt systemPrompt) {
    final parts = <String>[];
    final contextJson = systemPrompt.toJson();

    // Agregar contexto usando serializaci√≥n correcta
    final contextData = contextJson['context'];
    if (contextData != null) {
      final contextStr =
          contextData is String ? contextData : jsonEncode(contextData);
      if (contextStr.isNotEmpty && contextStr != '{}') {
        parts.add(contextStr);
      }
    }

    // Agregar instrucciones si hay
    if (systemPrompt.instructions.isNotEmpty) {
      final instructionsStr = jsonEncode(systemPrompt.instructions);
      if (instructionsStr.isNotEmpty && instructionsStr != '{}') {
        parts.add(instructionsStr);
      }
    }

    return parts.join(' ');
  }
}
